\documentclass[utf8]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage[autolanguage]{numprint}
\usepackage{float}
\usepackage{booktabs}
\usepackage{etoolbox}
\usepackage{siunitx}
\usepackage{natbib}
\usepackage{url}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\robustify\bfseries
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepgfplotslibrary{external}
\tikzexternalize  % allocate more memory if need be
\pgfplotsset{compat=newest}

\newcommand\men{\texttt{MEN}}
\newcommand\simlex{\texttt{SimLex}}
\newcommand\simverb{\texttt{SimVerb}}
\newcommand\sts{\texttt{STS2012}}
\newcommand\ws{\texttt{WS353}}
\newcommand\addreduce{\texttt{addreduce}}
\newcommand\glove{\texttt{GloVe}}
\newcommand\wordnet{\texttt{WordNet}}
\newcommand\fourlang{\texttt{4lang}}
\newcommand\paragram{\texttt{Paragram}}
\newcommand\wikiextractor{\texttt{Wikiextractor}}
\newcommand\polyglot{\texttt{Polyglot}}


\title{Dim Paper Status Report}
\author{Alexandre Kabbach}
\date{\today}

\begin{document}
\maketitle

\sisetup{detect-weight=true,detect-inline-weight=math}

All experiments are reported for $mincount=300$, $k=\numprint{10000}$ and
$kfoldsize=.20$ (20\%).


\section{Summary}

\subsection{What did we try?}
We tried sampling dimensions among $k=\numprint{10000}$ singular vectors extracted from the SVD of the PPMI matrix. We also tried alternative to SVD, namely ICA and NMF, but results are still pending.

\subsection{What did we find?}
\begin{enumerate}
  \item Our method can improve slightly over a standard SVD-top-k400 model. Usually all metrics can be improved by a margin of $\pm.02$.
  \item We can obtain SOTA RMSE scores on MEN. For the other datasets, results are still pending.
  \item Overall, our experiments showed that SPR was a limited evaluation metric: there is a tradeoff to be found between SPR and RMSE.
  \item The starting SVD model is important: we can accentuate the effects found in the different models, but not compensate them.
\end{enumerate}

\subsection{Todo}
\begin{enumerate}
  \item Perform the qualitative analysis of Ludovica on all our model dimension sets: try and compare those dimensions to find patterns as in the ACL paper
  \item Try and optimize on all datasets at once
  \item Add a couple of downstream tasks and measure what is most beneficial: high SPR or low RMSE. Check especially with overfit models (high SPR but also high RMSE). Could we explain the failure to predict task performance on downstream tasks this way?
\end{enumerate}

\subsection{Possible take on the paper}
Dimension sampling only brings limited improvement. It shows that, to some extend, variance preservation is not fully relevant to characterizing lexical semantics. Nonetheless, dimension sampling is limited to the dimensions it can sample, therefore to the latent aggregation of the data. Hence the need to compare with ICA and NMF, especially on \simlex\ and \simverb. Indeed, most approaches that do not make use of external resources (via, e.g., retrofitting) seem to work only on \men. Do they actually overfit on \men?
In absolute, the combination of PPMI + SVD-top (or any other SGNS equivalent with NMF) seems to work on \men, and only on \men. Our preliminary results on \simverb\ using sampling on the raw count matrix (no PPMI) with ICA and NMF give use equivalent performance in absolute scores (needs to be confirmed).


\section{Baseline and SOTA}

\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l*{6}{S[table-auto-round, table-format=1.2]}}
     \toprule
        & \multicolumn{2}{c}{MEN} & \multicolumn{2}{c}{SIMLEX} & \multicolumn{2}{c}{SIMVERB}\\
        Model & \multicolumn{1}{c}{spr} & \multicolumn{1}{c}{rmse} & \multicolumn{1}{c}{spr} & \multicolumn{1}{c}{rmse} & \multicolumn{1}{c}{spr} & \multicolumn{1}{c}{rmse} \\
        \midrule
        SVD-k400-alpha0-win2 & 0.75	& 0.33	& 0.41	& 0.31	& 0.27	& 0.35\\
        SVD-k400-alpha1-win2 & 0.68&0.20&0.33&0.28&0.19&\textbf{0.28}\\
        SVD-k400-alpha0-win5 & 0.77&0.32&0.38&0.32&0.25&0.36\\
        SVD-k400-alpha1-win5 & 0.69&\textbf{0.18}&0.28&0.29&0.16&0.29\\
        W2V-SG-d400 & 0.7&0.26&0.39&0.26&0.28&\textbf{0.28}\\
        Paragram~\citep{wietingetal2015} & 0.78&0.36&0.68&\textbf{0.24}&0.54&\textbf{0.28}\\
        CN~\citep{liuetal2018} & \textbf{0.84}&0.28&0.49&0.26&0.36&0.30\\
        SOTA & 0.84	& &	\textbf{0.76}	&	& \textbf{0.63}\\
     \bottomrule
    \end{tabular}
    }
    \caption{Baseline and SOTA}
\end{table}

SOTA on \simlex\ is the system of \cite{recskietal2016}. SOTA on \simverb\ is the system of \cite{mrkvsicetal2016} as reported in \simverb\ supplementary materials.
The model of \cite{recskietal2016} is an extension of the paragram embeddings of \cite{wietingetal2015}.
The model of \cite{mrkvsicetal2016} applied counter-fitting to the same paragram embeddings.

\section{MEN}
\input{results.men.table}
\input{results.men.plots}

\section{SIMLEX}
\input{results.simlex.table}
\input{results.simlex.plots}

\section{SIMVERB}
\input{results.simverb.table}
\input{results.simverb.plots}

\section{What next?}

\subsection{Frontiers}
We could probably keep the work done on \sts\ for the Frontiers paper: we optimize on \sts\ (with kfold), check both SPR and RMSE, and compare the models generated (intrinsically and extrinsically with word similarity datasets).
Rational: show that two speakers may agree on the semantics of a composed utterance (the sentence) while not having completely overlapping latent semantic models at the unit (word) level. Show nonetheless that, in order to agree at the sentence level, an above-random level of structure overlap (must/does?) exist(s). Show thereby that speakers end up having similar meaning spaces.

\subsection{Testing alternative latent feature aggregation methods}
What we are doing with SVD is somehow similar to clustering of latent features.
The literature on clustering show how diverse such clustering methods can be and that clustering is somehow subjective. First, we need to try ICA and NMF with sampling, especially on \simlex\ and \simverb\ as their absolute scores (especially SPR) remains relatively low in comparison to MEN. We may be able to achieve similar performance with ICA (as preliminary results suggest)

\subsection{Stability of the latent feature aggregation methods and robusteness to noise}
In light of recent work on the (in-)stability of word embeddings~\citep{antoniakandmimno2018,pierrejeanandtanguy2018}, the next question that arises is how sensitive our different methods for aggregating latent features are to missing data.
What if we were to replace a 0 by a 1 in the raw count matrix. More specifically, we must understand:
\begin{enumerate}
  \item the sensitivity to noise of the \emph{attention} mechanism (PPMI)
  \item the sensitivity to noise of the aggregation method itself (SVD, ICA, NMF)
\end{enumerate}
See previous work on replacing 0 by 1 in the original matrix. Then we could push further by introducing `realistic' linguistic noise. That is, retain the zipfian distribution of context counts but make it vary within such a distribution, and see qualitatively how the nature of the latent features change. The benefit of such an approach is that, as \cite{antoniakandmimno2018,pierrejeanandtanguy2018}, we can solely rely on intrinsic evaluation metrics.

\subsection{Increasing the number of constraints via matrix completion}
SPR-based optimization is prone to overfitting, which would suggest that there is too much flexibility in the SVD singular vectors, given the set of constraints, to actually capture anything robust and relevant.
Alternative RMSE-based methods help only to some extent: they can maintain the RMSE low on both train and test sets, show a high degree of correlation between train and test performance on both SPR and RMSE (no overfitting), but at the expense of no-so-high SPR scores.

One possiblity would be to explore increasing the number of constraints contained in each similarity dataset via matrix completion.\footnote{\url{https://en.wikipedia.org/wiki/Matrix_completion}}
Scikit-learn has a naive impletation for it via Imputers.\footnote{\url{https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py}}
But there is also a readily-available python package\footnote{\url{https://github.com/iskandr/fancyimpute}} especially for the NuclearNormMinimization.


\subsection{Supervised latent feature aggregation}
In a sense, what we are trying to do is hoping that the way our feature aggregation method does aggregate latent features will end up being relevant for the characterization of the semantic space, especially as per its evaluation on lexical similarity tasks. This seems rather optimistic when done in a fully unsupervised fashion.
Overall, we may wonder if it would not actually make more sense to introduce some form of \emph{semi-supervied clustering} where we impose certain constraints on the structure of the latent features, that is, of the word-vector matrix.

Consider a standard NMF problem where we are trying to factorize a given count (raw or PPMI) matrix $R$ into two matrices: $M$ and $H$, where $M$ will be the final (dim-reduced) vector matrix. $dim(R) = (n,m)$, $dim(M)=(n,k)$, $dim(H)=(k,m)$. The objective function is:
\begin{equation}
  \argmin_{M,H} ||R-MH||
\end{equation}

What we would like to do is add a constraint to $M$, given the similarity matrix $S$ (matrix, potentially completed, of human-based similarity judgements):
\begin{equation}
  \argmin_{M} ||S-PM||
\end{equation}
where $P$ is a $(n,n)$ projection matrix:
\begin{equation}
  P_{n,n} =
 \begin{pmatrix}
  I_{p,p} & 0_{p,n-p} \\
  0_{n-p,p} & 0_{n-p,n-p} \\
 \end{pmatrix}
\end{equation}
$S$ is a symmetric matrix of the form:
\begin{equation}
  S_{n,m} =
  \begin{pmatrix}
   S_{p,p} & 0_{p,m-p} \\
   0_{n-p,p} & 0_{n-p,m-p} \\
  \end{pmatrix}
\end{equation}
where $I$ is a diagonal ``identity'' matrix with 1 only.
Side note.\footnote{If the similarity matrix is positive-definite (or semi-definite) we could use the
Cholesky decomposition: \url{https://en.wikipedia.org/wiki/Cholesky_decomposition}}


\subsection{Looking for primitives...}
Just putting it here for future reference as sparse coding\footnote{\url{https://scikit-learn.org/stable/auto_examples/decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py}}\footnote{\url{https://stats.stackexchange.com/questions/172748/sparse-coding-with-constraints-in-the-optimization}} really seems like
looking for primitives to me...
It has also been tried for word embeddings: \citep{faruquietal2015,berend2017}.

\bibliography{results}
\bibliographystyle{acl_natbib}

\end{document}
